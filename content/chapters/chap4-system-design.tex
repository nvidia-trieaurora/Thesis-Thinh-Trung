\chapter{Detailed System Design}
\label{chap:system_design}

This chapter provides a comprehensive exposition of the system's detailed design, articulating the architectural decisions, methodologies, and technological implementations that underpin the platform. The design philosophy centers on leveraging a modern, serverless architecture to achieve scalability, maintainability, and rapid development. Each section adopts a problem-solution paradigm, presenting a specific technical challenge encountered during the development lifecycle, followed by a detailed description of the implemented solution, its rationale, and its advantages.

\section{System Architecture Overview}
\label{sec:system_architecture_overview}

The platform's architecture is a distributed system comprising several loosely coupled components, orchestrated to deliver a seamless data annotation experience. The high-level architecture, as depicted in Figure \ref{fig:system_architecture}, segregates responsibilities into four primary domains: the client-side frontend, the serverless backend, external service integrations, and a reverse proxy layer for routing and security.

\begin{figure}[h]
    \centering
    \includegraphics[width=\textwidth]{path/to/your/architecture-diagram.pdf}
    \caption{High-Level System Architecture Diagram.}
    \label{fig:system_architecture}
\end{figure}

The frontend is a single-page application (SPA) built with React, responsible for all user interactions. The backend is implemented entirely on the Supabase platform, utilizing its PostgreSQL database, authentication services, and real-time capabilities. This backend-as-a-service (BaaS) model obviates the need for traditional server management. Key external integrations, such as the OHIF Viewer for DICOM rendering and MONAI Label for AI-assisted annotation, are incorporated to provide specialized functionality. An Nginx reverse proxy serves as the entry point to the system, managing request routing and centralizing authentication concerns. The subsequent sections will deconstruct this architecture, providing a granular analysis of each component's design.

\section{Frontend Architecture}
\label{sec:frontend_architecture}

The frontend architecture is engineered for rapid development, maintainability, and a high-quality user experience. The design addresses two fundamental challenges in modern web application development: the overhead of building standard application features and the complexity of client-side state management.

\subsection{Accelerating Development with a Headless Framework}
\label{subsec:accelerating_development}

\subsubsection{Problem: Boilerplate and Scaffolding Overhead}
Developing a data-intensive application necessitates a significant amount of boilerplate code. For each distinct data entity—such as projects, tasks, or users—a developer must manually implement a suite of standard features: a view to list all items, a form to create a new item, a view to display an item's details, a form to edit it, and the logic to delete it. This pattern, commonly known as Create, Read, Update, Delete (CRUD), extends beyond data operations to include user authentication flows, system notifications, navigation menus, and breadcrumbs.

Without a structured framework, this leads to a proliferation of repetitive code. A developer might write a custom hook or component for fetching project data, another for task data, and so on. Each implementation would require its own state management (`useState`), effect handling (`useEffect`), and API call logic, as illustrated below. This approach is not only time-consuming but also highly susceptible to inconsistencies and bugs.

\begin{verbatim}
// Repetitive, manual data fetching for a single resource
function ProjectList() {
    const [projects, setProjects] = useState([]);
    const [loading, setLoading] = useState(true);
    const [error, setError] = useState(null);

    useEffect(() => {
        setLoading(true);
        api.get('/projects')
            .then(response => setProjects(response.data))
            .catch(err => setError(err))
            .finally(() => setLoading(false));
    }, []);

    // ... render logic for loading, error, and data states
}

// This entire block of code would need to be duplicated and
// adapted for tasks, users, and every other resource.
\end{verbatim}

The primary challenge was to eliminate this redundancy and abstract these common patterns in a way that accelerates development without sacrificing the flexibility needed for a custom user interface and complex business logic.

\subsubsection{Solution: Adoption of the Refine.dev Framework}
The platform leverages the \texttt{refine.dev} framework, a headless, React-based framework designed specifically to address the challenges of building data-intensive applications. Its core architectural concept is the \textbf{resource}. A resource is a declarative definition of a data entity within the application. By defining a resource, the developer provides \texttt{refine.dev} with the necessary metadata to automate a wide array of functionalities.

For instance, declaring a "projects" resource:
\begin{verbatim}
<Refine
    dataProvider={...}
    resources={[
        {
            name: "projects",
            list: "/projects",
            create: "/projects/create",
            edit: "/projects/edit/:id",
            show: "/projects/show/:id",
            meta: { canDelete: true }
        }
        // ... other resources
    ]}
>
    {/* ... */}
</Refine>
\end{verbatim}
This single definition unlocks a suite of powerful, interconnected features:
\begin{itemize}
    \item \textbf{Data Hooks}: It enables the use of high-level data hooks like `useList` for fetching collections, `useOne` for single items, `useCreate` for mutations, etc., which are pre-configured to communicate with the backend `dataProvider`.
    \item \textbf{Automated Routing}: The framework automatically generates the necessary routes for the list, create, edit, and show pages.
    \item \textbf{UI Generation}: When paired with a UI library like Ant Design, `refine.dev` can automatically generate side-menu navigation links and breadcrumbs for each resource, ensuring a consistent user experience.
    \item \textbf{Authentication and Authorization}: The `meta` field can be used to integrate with the `accessControlProvider`, allowing for granular control over which users can perform actions like deleting a resource.
\end{itemize}
This resource-centric, declarative approach abstracts away the repetitive, low-level implementation details of CRUD operations and application infrastructure. It allowed the development team to focus engineering effort on high-value, domain-specific features, such as the workflow builder and the integrated commenting system, thereby dramatically increasing development velocity and improving code consistency.

\subsection{Efficient Data Caching and State Synchronization}
\label{subsec:efficient_caching}

\subsubsection{Problem: Inefficient Data Fetching and UI Inconsistency}
In any complex single-page application, managing server state—data fetched from a remote source—is a formidable challenge. Traditional approaches often lead to significant performance issues and a brittle codebase. Two common anti-patterns illustrate this problem:

\begin{enumerate}
    \item \textbf{Component-Level Fetching}: The most straightforward approach is for each component to fetch its own data. This leads to request waterfalls, where a parent component fetches data and then a child component, upon rendering, initiates its own fetch. This creates perceptible latency. Furthermore, if multiple, unrelated components on the same page require the same data (e.g., the current user's profile), they will each make a redundant API call for that data, as shown in Figure \ref{fig:request_waterfall}.

    \begin{figure}[h]
        \centering
        \includegraphics[width=0.8\textwidth]{path/to/your/request-waterfall-diagram.pdf}
        \caption{Redundant API calls from independent components.}
        \label{fig:request_waterfall}
    \end{figure}

    \item \textbf{Manual Caching with Global State}: To solve the redundancy issue, developers often turn to global state management libraries like Redux or the Context API. Data is fetched once and stored in a central store. While this prevents duplicate requests, it introduces immense boilerplate complexity. For each API endpoint, one must write actions, reducers, selectors, and thunks/sagas to handle the fetching, loading, success, and error states. More critically, the cache becomes stale. If a user updates their profile in one part of the app, it is the developer's manual responsibility to ensure the cache is updated and all subscribed components are re-rendered. This manual cache invalidation is a frequent source of bugs, leading to inconsistent and outdated UIs.
\end{enumerate}

\subsubsection{Solution: Declarative Caching and Automated Invalidation with TanStack Query}
The \texttt{refine.dev} framework addresses these challenges by integrating TanStack Query as its default server-state management engine. This library provides a robust, out-of-the-box solution that automates caching, background updates, and state synchronization.

When a data hook like `useList("projects")` is called, TanStack Query fetches the data and stores it in a global, in-memory cache under a unique query key (`["projects"]`). Subsequent calls to `useList("projects")` from any other component will instantly receive the cached data, eliminating redundant network requests. The behavior of this cache is controlled via two key parameters:
\begin{itemize}
    \item \textbf{\texttt{staleTime}}: Dictates how long cached data is considered "fresh." Requests made within this window are served from the cache without a network call.
    \item \textbf{\texttt{cacheTime}}: Dictates how long inactive data is kept in memory before being garbage collected.
\end{itemize}

The most significant advantage of this approach is its automated, declarative strategy for data synchronization. Instead of manually refetching data after a change, the system uses \textbf{query invalidation}.
When a mutation occurs (e.g., a user updates a project via the `useUpdate` hook), `refine`'s data provider automatically calls `queryClient.invalidateQueries(['projects'])`. This action does not immediately trigger a refetch. Instead, it marks all data associated with the `['projects']` query key as stale. TanStack Query then intelligently and automatically refetches the data for any currently mounted component that is subscribed to that query.

This strategy avoids the anti-pattern of "prop drilling" a `refetch` function down through multiple component layers. The component triggering the mutation does not need to know which other components display the data. It simply invalidates the relevant piece of data, and the framework ensures that the UI updates consistently across the entire application. This declarative approach radically simplifies state management, reduces bugs, and improves application performance and resilience.

\section{Backend Architecture and Workflow Orchestration}
\label{sec:backend_architecture}

The backend architecture is founded on the principle of a "logic-intensive database," wherein complex business rules, data access policies, and asynchronous processes are encapsulated directly within PostgreSQL. This approach, facilitated by Supabase, minimizes external dependencies and centralizes the system's core logic.

\subsection{Secure and Abstracted Data Access via Views and Functions}
\label{subsec:secure_data_access}

\subsubsection{Problem: Direct Table Exposure and Logic Duplication}
Exposing raw database tables directly via an API is a significant security risk and a poor architectural practice. It tightly couples the frontend application to the physical data schema, making future schema migrations brittle and difficult. Moreover, business logic and validation rules (e.g., "a new project must be assigned to the user who created it") would need to be re-implemented in every client or middleware that interacts with the data, leading to code duplication and inconsistencies.

\subsubsection{Solution: A Multi-Layered Abstraction with PostgreSQL Views and Functions}
To address this, the backend employs a robust abstraction layer within the database itself.
\begin{enumerate}
    \item \textbf{Core Tables}: The fundamental data is stored in "private" tables, prefixed with an underscore (e.g., `_projects`, `_tasks`). These tables are never directly exposed to the API.
    \item \textbf{Security Views}: For data retrieval (Read operations), PostgreSQL views are created (e.g., `projects`, `tasks`). These views act as a stable interface for the frontend, abstracting the underlying table structure. They are defined with `WITH (security_invoker = true)` and are governed by Supabase's Row-Level Security (RLS) policies, ensuring that users can only see the data they are permitted to access.
    \item \textbf{SQL Functions as Atomic Transactions}: For data mutation, PostgreSQL functions are implemented. For standard CRUD operations, these functions follow a strict naming convention (e.g., `projects_create`, `projects_update`, `projects_delete`). The `refine.dev` data provider is configured to call these functions by convention, mapping a `create` operation on the `projects` resource directly to the `projects_create` SQL function.
\end{enumerate}

For more complex actions that do not fit the standard CRUD model, such as "submitting a task for review" or "initiating a consensus check," the platform leverages `refine.dev`'s `useCustomMutation` hook. This hook allows the frontend to directly invoke a specific SQL function via RPC without needing to be tied to a resource's standard `create` or `update` methods. This provides the flexibility to handle any business process while ensuring that all logic remains centralized, atomic, and secure within the database. This design decouples the schema from the API, enforces security at the data layer, and executes business logic atomically and consistently.

\subsection{Asynchronous Workflow Automation Engine}
\label{subsec:workflow_automation}

\subsubsection{Problem: Orchestrating Complex, Asynchronous State Transitions}
Data annotation workflows are inherently stateful and often involve asynchronous, non-human steps. For example, after two annotators complete a task, a consensus algorithm must run. Or, a task might need to be automatically routed to an AI model for pre-labeling. Implementing a reliable system to manage these state transitions is non-trivial. A traditional approach would require a dedicated, always-on server application with a job queue and workers, which contradicts the serverless architecture and adds significant operational complexity.

\subsubsection{Solution: Cron-Driven Orchestration via Virtual Users}
The platform implements a novel workflow engine entirely within PostgreSQL, driven by scheduled cron jobs. This engine operates on the concept of "virtual users," which are system accounts (e.g., `ROUTER`, `CONSENSUS`) that represent automated processes.

The orchestration flow is illustrated in Figure \ref{fig:workflow_engine_flow} and proceeds as follows:
\begin{enumerate}
    \item \textbf{State Representation}: The entire workflow is modeled as a directed graph, defined in the `_workflows` and `_workflow_stages` tables. The visual representation of this graph is created on the frontend using the \texttt{xyflow} library.
    \item \textbf{Assignment to Virtual User}: When a task reaches a stage requiring automation (e.g., a 'ROUTER' stage), a new entry is created in the `_task_assignments` table, assigning the task to the corresponding virtual user (e.g., the `ROUTER` user ID).
    \item \textbf{Scheduled Polling}: A PostgreSQL cron job, such as `workflow_route()`, is scheduled to execute periodically (e.g., every minute).
    \item \textbf{Task Processing}: The cron function queries the `_task_assignments` table for pending tasks assigned to its designated virtual user.
    \item \textbf{Logic Execution}: For each pending task, the function executes its domain-specific logic. For a `ROUTER` stage, it reads the routing rules from the stage's `custom_config` JSONB field and determines the next stage.
    \item \textbf{State Transition}: The function marks the current assignment as 'COMPLETED' and creates a new assignment for the next stage in the workflow. This creates an immutable, auditable log of the entire workflow execution.
\end{enumerate}

\begin{figure}[h]
    \centering
    \includegraphics[width=0.9\textwidth]{path/to/your/workflow-engine-flow.pdf}
    \caption{State Transition Diagram of the Cron-Driven Workflow Engine.}
    \label{fig:workflow_engine_flow}
\end{figure}

This cron-based design provides a robust, scalable, and fully automated workflow engine without the need for any external server infrastructure, perfectly aligning with the project's serverless ethos.

\section{Infrastructure and Security}
\label{sec:infrastructure_security}

\subsection{Unified Authentication Across Micro-Frontends}
\label{subsec:unified_authentication}

\subsubsection{Problem: Disparate Authentication States}
The system's architecture integrates the OHIF Viewer as an embedded component, which effectively acts as a micro-frontend within the main application. A naive implementation would lead to significant authentication challenges. Each "frontend" (the main app and the viewer) would need to manage its own authentication state. This could require the user to log in twice or involve complex, insecure token-passing mechanisms (e.g., using `postMessage` to send a JWT to the viewer's iframe). As depicted in Figure \ref{fig:disparate_auth}, this approach creates a fragmented user experience and enlarges the attack surface for vulnerabilities like token interception.

\begin{figure}[h]
    \centering
    \includegraphics[width=\textwidth]{path/to/your/disparate-auth-diagram.pdf}
    \caption{Problematic Disparate Authentication Architecture.}
    \label{fig:disparate_auth}
\end{figure}

\subsubsection{Solution: Centralized Authentication via Nginx Reverse Proxy}
The chosen solution was to centralize authentication at the infrastructure level using Nginx as a reverse proxy and authentication gateway. This pattern, shown in Figure \ref{fig:unified_auth}, removes the burden of authentication enforcement from the individual client applications.

\begin{figure}[h]
    \centering
    \includegraphics[width=\textwidth]{path/to/your/unified-auth-diagram.pdf}
    \caption{Centralized Authentication via Nginx Reverse Proxy.}
    \label{fig:unified_auth}
\end{figure}

The request flow is as follows:
\begin{enumerate}
    \item The user's browser makes a request to any part of the application.
    \item Nginx, as the sole entry point, intercepts the request.
    \item It inspects the request for the Supabase JWT stored in a secure, HttpOnly cookie.
    \item Nginx uses the `auth_request` directive to make a subrequest to an internal endpoint that validates the JWT against Supabase's `/auth/v1/user` endpoint.
    \item \textbf{If the token is valid}, the subrequest returns a 200 OK status, and Nginx proxies the original request to the appropriate upstream service (the main React application or the OHIF viewer).
    \item \textbf{If the token is invalid or absent}, the subrequest returns a 401 Unauthorized status, and Nginx immediately redirects the user to the login page without ever touching the downstream applications.
\end{enumerate}
This configuration simplifies the frontend code immensely. Neither the main application nor the OHIF integration needs to contain logic to protect its routes; they can trust that any request they receive has already been authenticated by the Nginx gateway. This creates a more secure, maintainable, and robust authentication system.
