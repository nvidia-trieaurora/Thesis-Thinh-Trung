\chapter{Literature Review}
\label{chap:literature-review}

\begin{ChapAbstract}
This chapter provides a comprehensive review of existing medical imaging annotation platforms, workflow management systems, and AI-assisted annotation approaches. We examine the current state of medical image annotation tools, with particular focus on DICOM viewers and AI-integrated annotation systems. We analyze their limitations in workflow flexibility, access control, and AI integration, exploring relevant techniques including graph-based workflow models, Attribute-Based Access Control (ABAC), and Model-in-the-Loop approaches for medical imaging. The chapter concludes by identifying gaps in existing solutions that motivate the proposed platform architecture combining OHIF viewer with MONAI Label Server.
\end{ChapAbstract}

\section{Medical Image Annotation and DICOM Viewers}
\label{sec:medical-annotation-intro}

Medical image annotation represents a specialized and critical domain within data annotation, where the quality and accuracy of annotations directly impact clinical decision-making and AI model development for healthcare applications. Unlike general-purpose annotation tasks, medical imaging annotation requires specialized knowledge, adherence to medical standards, and integration with healthcare infrastructure \cite{litjens2017survey}.

\subsection{Medical Imaging Data Standards and Formats}

Medical imaging relies heavily on standardized formats and protocols:

\begin{itemize}
    \item \textbf{DICOM (Digital Imaging and Communications in Medicine)}: The international standard for medical imaging data storage, transmission, and viewing \cite{pianykh2012digital}
    \item \textbf{Medical Image Segmentation}: Precise delineation of anatomical structures, organs, lesions, and pathological regions
    \item \textbf{Radiological Annotation}: Structured reporting following standards like RSNA RadLex and BI-RADS for breast imaging
    \item \textbf{Multi-modal Imaging}: Integration of different imaging modalities (CT, MRI, PET, ultrasound) requiring specialized annotation approaches
    \item \textbf{3D/4D Annotation}: Volumetric and temporal annotation for advanced medical imaging studies
\end{itemize}

\subsection{Challenges in Medical Image Annotation Workflows}

Medical image annotation workflows face unique challenges beyond traditional data annotation:

\begin{enumerate}
    \item \textbf{Clinical Expertise Requirements}: Medical imaging annotation requires radiologists, clinicians, or specially trained medical professionals, creating significant bottlenecks in annotation pipelines.
    
    \item \textbf{Regulatory Compliance}: Medical annotation must comply with healthcare regulations (HIPAA, GDPR) and medical device standards (FDA 510(k), CE marking).
    
    \item \textbf{Quality Assurance}: Inter-observer variability in medical annotations requires sophisticated consensus mechanisms and quality control measures.
    
    \item \textbf{AI Integration}: Modern medical annotation increasingly relies on AI assistance for pre-annotation, quality checking, and active learning approaches.
    
    \item \textbf{DICOM Infrastructure}: Integration with existing hospital PACS systems, DICOM servers, and medical imaging infrastructure.
    
    \item \textbf{Workflow Complexity}: Medical annotation often requires multi-stage workflows involving screening, annotation, review, and clinical validation.
\end{enumerate}

\section{Medical Imaging Platforms and DICOM Viewers}
\label{sec:medical-platforms}

\subsection{Open Source Medical Imaging Viewers}

\subsubsection{OHIF (Open Health Imaging Foundation) Viewer}

OHIF \cite{ohif2023} represents the leading open-source, web-based DICOM viewer designed for medical imaging workflows:

\begin{itemize}
    \item \textbf{Web-based Architecture}: Zero-footprint deployment using modern web technologies (React, Cornerstone.js)
    \item \textbf{DICOM Compliance}: Full support for DICOMweb protocols (WADO-RS, QIDO-RS, STOW-RS)
    \item \textbf{Multi-modal Support}: Comprehensive support for CT, MRI, PET, ultrasound, and microscopy imaging
    \item \textbf{Plugin Architecture}: Extensible framework supporting custom tools, hanging protocols, and data sources
    \item \textbf{Measurement Tools}: Built-in annotation tools for length, area, angle measurements, and ROI analysis
    \item \textbf{MPR (Multi-Planar Reconstruction)}: Advanced 3D visualization and cross-sectional analysis
\end{itemize}

\textbf{Strengths}: OHIF's extensible architecture and standards compliance make it ideal for integration with AI annotation workflows. Its web-based nature eliminates installation requirements while providing enterprise-grade viewing capabilities.

\textbf{Limitations}: Limited built-in workflow management and basic annotation features compared to specialized annotation platforms. Requires additional infrastructure for complex annotation workflows.

\subsubsection{Cornerstone.js Ecosystem}

Cornerstone.js \cite{cornerstone2023} provides the foundational medical imaging framework underlying OHIF:

\begin{itemize}
    \item High-performance medical image rendering and manipulation
    \item Support for various medical image formats and transfer syntaxes
    \item Tool framework for measurement and annotation
    \item Integration with medical imaging standards
\end{itemize}

\subsection{AI-Integrated Medical Annotation Platforms}

\subsubsection{MONAI Label Server}

MONAI Label \cite{monai2023} represents a cutting-edge server-client system for AI-assisted medical image annotation:

\begin{itemize}
    \item \textbf{AI-Assisted Annotation}: Deep learning models for automated pre-annotation and suggestion generation
    \item \textbf{Active Learning}: Intelligent sample selection for efficient annotation workflows
    \item \textbf{Model Integration}: Support for PyTorch and MONAI-based models with easy deployment
    \item \textbf{Multi-Model Pipeline}: Support for localization, segmentation, and classification models in unified workflows
    \item \textbf{Client Integrations}: Native plugins for 3D Slicer, OHIF, and other medical imaging platforms
    \item \textbf{Continuous Learning}: Model improvement through iterative training on annotated data
\end{itemize}

\textbf{Model Types Supported}:
\begin{itemize}
    \item \textbf{Segmentation Models}: Organ and lesion segmentation (spleen, vertebra, organs)
    \item \textbf{Localization Models}: Anatomical structure localization (spine, vertebra detection)
    \item \textbf{DeepEdit}: Interactive segmentation refinement with user guidance
    \item \textbf{DeepGrow}: Click-based segmentation for rapid annotation
\end{itemize}

\subsubsection{Integration Architecture: OHIF + MONAI Label}

The combination of OHIF viewer with MONAI Label Server creates a powerful AI-assisted annotation platform:

\begin{itemize}
    \item \textbf{Seamless Workflow}: Users interact with AI models directly within the OHIF interface
    \item \textbf{Real-time AI Assistance}: Immediate model predictions and refinement suggestions
    \item \textbf{Quality Enhancement}: AI pre-annotations reduce annotation time and improve consistency
    \item \textbf{Standards Compliance}: Full DICOM compatibility maintained throughout the annotation process
\end{itemize}

\subsection{Comparative Analysis of Medical Imaging Platforms}

Table \ref{tab:medical-platform-comparison} summarizes the key capabilities and limitations of existing medical imaging annotation platforms:

\begin{table}[htbp]
\centering
\caption{Comparison of Medical Imaging Annotation Platforms}
\label{tab:medical-platform-comparison}
\begin{tabular}{|l|c|c|c|c|c|}
\hline
\textbf{Platform} & \textbf{DICOM Support} & \textbf{AI Integration} & \textbf{Workflow Mgmt} & \textbf{Web-based} & \textbf{Extensibility} \\
\hline
OHIF Viewer & Excellent & Limited & Basic & Yes & High \\
3D Slicer & Good & MONAI Plugin & Limited & No & High \\
MONAI Label & N/A & Excellent & Basic & Server-only & High \\
Cornerstone.js & Excellent & None & None & Yes & High \\
RadiAnt & Good & None & None & No & Limited \\
\hline
\end{tabular}
\end{table}

\textbf{Key Findings}:
\begin{itemize}
    \item \textbf{OHIF + MONAI Label combination} addresses the gap between advanced DICOM viewing and AI-assisted annotation
    \item Most existing platforms lack comprehensive workflow management for complex annotation pipelines
    \item Web-based deployment is crucial for enterprise adoption and integration
    \item AI integration varies significantly, with MONAI Label leading in medical AI capabilities
\end{itemize}

\section{Related Techniques and Models}
\label{sec:related-techniques}

\subsection{Workflow Models for Medical Annotation}
\label{subsec:workflow-models}

\subsubsection{Linear vs. Graph-Based Workflow Models}

Traditional annotation platforms employ simple linear workflows where tasks progress sequentially through fixed stages. However, medical annotation requires more sophisticated orchestration due to its inherent complexity and quality requirements.

\textbf{Linear Workflow Limitations}:
\begin{itemize}
    \item Fixed sequence: Task 1 → Task 2 → Task 3 → Complete
    \item No conditional routing based on content complexity or AI confidence
    \item Limited support for consensus resolution and iterative refinement
    \item Inefficient handling of cases requiring expert review or AI assistance
\end{itemize}

\textbf{Graph-Based Workflow Advantages}:

Graph-based workflow models \cite{aalst2003workflow} provide the flexibility needed for complex medical annotation scenarios by representing workflows as directed graphs where:

\begin{itemize}
    \item \textbf{Stage-Based Processing}: Nodes represent distinct workflow stages (ANNOTATE, REVIEW, CONSENSUS, MITL, ROUTER)
    \item \textbf{Conditional Routing}: Dynamic routing based on annotation quality, consensus results, or AI confidence scores
    \item \textbf{Parallel Execution}: Multiple annotators working simultaneously on different aspects of the same study
    \item \textbf{Loop Support}: Iterative refinement processes with AI-assisted corrections and human validation
    \item \textbf{Error Handling}: Automatic fallback mechanisms when AI models fail or produce low-confidence results
\end{itemize}

\textbf{Practical Example - Spine Annotation Workflow}:

Consider a typical medical imaging workflow for spine annotation:

\begin{enumerate}
    \item \textbf{ROUTER}: Evaluates study complexity and AI model availability
    \item \textbf{MITL}: AI pre-annotates spine localization and vertebra segmentation
    \item \textbf{ANNOTATE}: Radiologist reviews and refines AI predictions
    \item \textbf{REVIEW}: Senior radiologist validates complex cases (based on AI confidence score < 0.8)
    \item \textbf{CONSENSUS}: Multiple experts resolve disagreements for research studies
    \item \textbf{ROUTER}: Routes to appropriate completion or additional review cycles
\end{enumerate}

\subsubsection{Medical Workflow Stage Types}

Our platform implements specialized workflow stages optimized for medical annotation:

\begin{itemize}
    \item \textbf{ANNOTATE}: Human annotation with optional AI pre-population
    \item \textbf{REVIEW}: Expert review and validation of annotations
    \item \textbf{CONSENSUS}: Multi-annotator consensus resolution
    \item \textbf{MITL (Model-in-the-Loop)}: AI-assisted annotation and quality checking
    \item \textbf{ROUTER}: Conditional logic for dynamic workflow routing
\end{itemize}

\subsubsection{Integration with Medical Imaging Infrastructure}

Workflow engines for medical annotation must integrate seamlessly with existing healthcare infrastructure:

\begin{itemize}
    \item \textbf{DICOM Server Integration}: Direct connectivity with Orthanc, dcm4che, and hospital PACS systems
    \item \textbf{HL7 FHIR Compliance}: Standard healthcare data exchange protocols
    \item \textbf{Audit Trail Requirements}: Complete tracking for regulatory compliance and medical device certification
\end{itemize}

\subsection{Access Control Models}
\label{subsec:access-control}

\subsubsection{Role-Based Access Control (RBAC)}

RBAC \cite{sandhu1996role} is the predominant access control model in current annotation platforms. Users are assigned roles, and permissions are granted to roles rather than individual users.

\textbf{Advantages}:
\begin{itemize}
    \item Simplified administration through role hierarchies
    \item Well-understood and widely implemented
    \item Suitable for stable organizational structures
\end{itemize}

\textbf{Limitations}:
\begin{itemize}
    \item Lacks context-awareness
    \item Difficult to model complex, dynamic permission requirements
    \item Role explosion problem in large, diverse organizations
\end{itemize}

\subsubsection{Attribute-Based Access Control (ABAC)}

ABAC \cite{hu2014guide} extends traditional access control by incorporating attributes of users, resources, and environmental context into authorization decisions.

\textbf{Key Components}:
\begin{itemize}
    \item \textbf{Subject Attributes}: User properties (role, department, clearance level)
    \item \textbf{Resource Attributes}: Data properties (classification, owner, project)
    \item \textbf{Action Attributes}: Operation properties (read, write, delete)
    \item \textbf{Environmental Attributes}: Context properties (time, location, network)
\end{itemize}

\textbf{Medical Domain Examples}:

\textbf{Subject Attributes} in healthcare contexts:
\begin{itemize}
    \item Role: radiologist, cardiology-fellow, medical-student, researcher
    \item Department: cardiology, oncology, emergency, research
    \item Experience level: junior (< 2 years), senior (2-10 years), expert (> 10 years)
    \item Certifications: board-certified, fellowship-trained, research-qualified
\end{itemize}

\textbf{Resource Attributes} for medical imaging data:
\begin{itemize}
    \item Patient sensitivity: public-dataset, anonymized-research, clinical-confidential
    \item Study type: routine-screening, emergency-trauma, research-trial, teaching-case
    \item Imaging modality: CT, MRI, PET, ultrasound, X-ray
    \item Anatomical region: cardiac, neurological, oncological, musculoskeletal
\end{itemize}

\textbf{Environmental Attributes} in medical settings:
\begin{itemize}
    \item Access location: hospital-workstation, home-vpn, mobile-device
    \item Time context: business-hours, emergency-hours, weekend-research
    \item Network security: secure-hospital-network, vpn-connection, public-wifi
\end{itemize}

\textbf{ABAC Policy Examples for Medical Annotation}:

\begin{enumerate}
    \item \textbf{Basic Annotation Access}:
    \begin{quote}
    \textit{ALLOW if (user.role = "radiologist" AND resource.sensitivity != "research-restricted" AND action = "annotate" AND environment.location = "hospital-network")}
    \end{quote}
    
    \item \textbf{Emergency Override}:
    \begin{quote}
    \textit{ALLOW if (user.department = "emergency" AND resource.study-type = "trauma" AND environment.time = "emergency-hours")}
    \end{quote}
    
    \item \textbf{Research Data Access}:
    \begin{quote}
    \textit{ALLOW if (user.clearance = "research-approved" AND resource.dataset = "anonymized" AND user.project-member = resource.project-id)}
    \end{quote}
\end{enumerate}

\textbf{Advantages}:
\begin{itemize}
    \item Fine-grained, context-aware access control
    \item Dynamic policy evaluation
    \item Scalable to complex organizational requirements
    \item Support for regulatory compliance
\end{itemize}

\textbf{Implementation Challenges}:
\begin{itemize}
    \item Increased complexity in policy definition and management
    \item Performance considerations for real-time policy evaluation
    \item Limited tooling and platform support
\end{itemize}

\subsection{AI-Assisted Medical Annotation Approaches}
\label{subsec:ai-assisted-annotation}

\subsubsection{Model-in-the-Loop (MITL) for Medical Imaging}

MITL approaches \cite{monarch2021human} have proven particularly effective in medical imaging annotation, where AI models can significantly accelerate the annotation process while maintaining clinical accuracy:

\begin{itemize}
    \item \textbf{AI Pre-annotation}: MONAI Label models provide initial segmentations for organs, lesions, and anatomical structures
    \item \textbf{Interactive Refinement}: DeepEdit and DeepGrow models enable real-time refinement based on user clicks and corrections
    \item \textbf{Quality Scoring}: AI confidence scores help prioritize cases requiring human review
    \item \textbf{Active Learning}: Models identify the most informative cases for annotation to improve training efficiency
\end{itemize}

\textbf{Concrete MITL Workflow Example - Vertebra Segmentation}:

\begin{enumerate}
    \item \textbf{AI Pre-annotation Phase}:
    \begin{itemize}
        \item MONAI spine localization model identifies spinal region in CT scan
        \item Vertebra detection model locates individual vertebrae (C1-C7, T1-T12, L1-L5)
        \item Segmentation model generates initial vertebra masks with confidence scores
    \end{itemize}
    
    \item \textbf{Human Review Phase}:
    \begin{itemize}
        \item Radiologist reviews AI predictions displayed in OHIF viewer
        \item High-confidence segments (> 0.9) highlighted in green (minimal review needed)
        \item Low-confidence segments (< 0.7) highlighted in red (requires attention)
        \item Intermediate confidence (0.7-0.9) highlighted in yellow (quick verification)
    \end{itemize}
    
    \item \textbf{Interactive Refinement}:
    \begin{itemize}
        \item DeepEdit: Radiologist clicks to indicate corrections → model instantly updates segmentation
        \item DeepGrow: Radiologist provides positive/negative clicks → model grows/shrinks segmentation
        \item Real-time feedback: Each correction immediately improves the current annotation
    \end{itemize}
    
    \item \textbf{Quality Assurance Loop}:
    \begin{itemize}
        \item AI quality checker evaluates final annotations for anatomical consistency
        \item Flags potential errors: missing vertebrae, overlapping segments, unrealistic shapes
        \item Routes questionable cases back to expert review or consensus workflow
    \end{itemize}
\end{enumerate}

\textbf{MITL vs. Traditional Annotation Comparison}:

\begin{table}[htbp]
\centering
\caption{MITL vs. Traditional Medical Annotation Workflow Comparison}
\label{tab:mitl-comparison}
\begin{tabular}{|l|c|c|}
\hline
\textbf{Aspect} & \textbf{Traditional Workflow} & \textbf{MITL Workflow} \\
\hline
Initial Annotation & Manual from scratch & AI pre-annotation \\
Time per Case & 45-60 minutes & 15-25 minutes \\
Consistency & Variable (inter-observer) & Higher (AI baseline) \\
Quality Control & Manual review only & AI + Human validation \\
Learning Integration & None & Continuous model improvement \\
Error Detection & Human-dependent & AI-assisted flagging \\
\hline
\end{tabular}
\end{table}

\textbf{Human-AI Collaboration Patterns}:

\begin{itemize}
    \item \textbf{AI-First}: Model provides initial annotation, human refines
    \item \textbf{Human-First}: Human starts annotation, AI assists with completion
    \item \textbf{Collaborative}: Real-time interaction where both human and AI contribute simultaneously
    \item \textbf{Consensus-Driven}: Multiple AI models + human experts reach consensus
\end{itemize}

\subsubsection{MONAI Label Integration Patterns}

MONAI Label Server enables sophisticated AI integration patterns in medical annotation workflows:

\begin{itemize}
    \item \textbf{Multi-Stage Pipelines}: Sequential application of localization, segmentation, and classification models
    \item \textbf{Model Chaining}: Vertebra pipeline combining spine localization, vertebra detection, and segmentation
    \item \textbf{Ensemble Methods}: Multiple model predictions combined for improved accuracy
    \item \textbf{Continuous Learning}: Models retrain automatically on newly annotated data
\end{itemize}

\subsubsection{Consensus Mechanisms in Medical Annotation}

Medical annotation consensus addresses inter-observer variability and ensures clinical validity \cite{sheng2008get}:

\begin{itemize}
    \item \textbf{Expert Consensus}: Senior radiologists resolve disagreements between junior annotators
    \item \textbf{AI-Assisted Consensus}: Model predictions provide additional input for consensus resolution
    \item \textbf{Confidence-Weighted Voting}: Incorporating both human confidence and AI certainty scores
    \item \textbf{Statistical Validation}: Inter-observer agreement metrics (Dice coefficient, Hausdorff distance) for quality assessment
\end{itemize}

\section{Identified Gaps and Proposed Solution}
\label{sec:problems-approach}

\subsection{Limitations in Current Medical Annotation Platforms}

Our analysis reveals critical gaps in existing medical imaging annotation solutions:

\subsubsection{Limited AI-Viewer Integration}

Current medical imaging platforms lack seamless AI integration:

\begin{itemize}
    \item Disconnected AI services requiring manual data transfer between systems
    \item Lack of real-time AI assistance within the viewing interface
    \item Limited support for interactive AI refinement during annotation
    \item Absence of standardized AI model integration protocols for medical imaging
\end{itemize}

\subsubsection{Inflexible Workflow Management}

Existing platforms cannot adapt to complex medical annotation requirements:

\begin{itemize}
    \item Linear workflows inadequate for consensus-based medical annotation
    \item Inability to route cases based on AI confidence scores or clinical complexity
    \item Limited support for multi-stage AI pipelines (localization → segmentation → classification)
    \item Lack of automatic fallback mechanisms when AI models fail
\end{itemize}

\subsubsection{Inadequate Access Control for Healthcare}

Current access control models cannot meet healthcare requirements:

\begin{itemize}
    \item Coarse-grained role-based permissions insufficient for multi-disciplinary teams
    \item Lack of context-aware access (patient data sensitivity, clinical protocols)
    \item Insufficient audit trails for regulatory compliance (HIPAA, GDPR)
    \item Limited support for dynamic permissions based on clinical workflow stages
\end{itemize}

\subsubsection{Poor DICOM Infrastructure Integration}

Most platforms operate in isolation from healthcare infrastructure:

\begin{itemize}
    \item Limited connectivity with hospital PACS and DICOM servers
    \item Lack of real-time synchronization with clinical imaging workflows
    \item Insufficient support for DICOM metadata and structured reporting
    \item Poor integration with existing medical imaging IT infrastructure
\end{itemize}

\subsection{Proposed Platform Architecture}

We propose a comprehensive medical annotation platform that addresses these limitations through four key innovations:

\subsubsection{Integrated OHIF-MONAI Architecture}

Our solution combines the strengths of OHIF viewer and MONAI Label Server:

\begin{itemize}
    \item \textbf{Seamless AI Integration}: MONAI Label models accessible directly within OHIF interface
    \item \textbf{Real-time Interaction}: Interactive segmentation refinement with DeepEdit and DeepGrow
    \item \textbf{Multi-Model Pipelines}: Support for complex AI workflows (spine → vertebra → segmentation)
    \item \textbf{Standards Compliance}: Full DICOM compatibility maintained throughout AI-assisted workflows
\end{itemize}

\subsubsection{Medical Workflow Engine}

Graph-based workflow management optimized for medical annotation:

\begin{itemize}
    \item \textbf{Stage-Based Processing}: Specialized stages (ANNOTATE, REVIEW, CONSENSUS, MITL, ROUTER)
    \item \textbf{AI-Driven Routing}: Dynamic routing based on model confidence and annotation quality
    \item \textbf{Consensus Mechanisms}: Multi-annotator consensus with expert resolution pathways
    \item \textbf{Error Handling}: Automatic fallback when AI models produce low-confidence results
\end{itemize}

\subsubsection{Healthcare-Grade Access Control}

Attribute-Based Access Control (ABAC) tailored for medical environments:

\begin{itemize}
    \item \textbf{Clinical Context}: Permissions based on patient data sensitivity and clinical protocols
    \item \textbf{Multi-Disciplinary Support}: Fine-grained roles for radiologists, technicians, researchers
    \item \textbf{Compliance Integration}: Comprehensive audit trails meeting HIPAA and medical device standards
    \item \textbf{Dynamic Authorization}: Context-aware permissions adapting to workflow progression
\end{itemize}

\subsubsection{DICOM Infrastructure Integration}

Native integration with medical imaging infrastructure:

\begin{itemize}
    \item \textbf{Orthanc Integration}: Direct connectivity with open-source DICOM servers
    \item \textbf{Real-time Synchronization}: Automatic study synchronization and metadata extraction
    \item \textbf{PACS Compatibility}: Standards-compliant integration with hospital imaging systems
    \item \textbf{Structured Reporting}: Support for DICOM SR and standard radiological reporting formats
\end{itemize}

This architecture provides a robust foundation for scalable, AI-assisted medical annotation while maintaining compliance with healthcare standards and regulations. The following chapters detail the implementation and evaluation of this integrated approach.

% References for this chapter
\begin{thebibliography}{99}

\bibitem{litjens2017survey}
Litjens, G., Kooi, T., Bejnordi, B. E., Setio, A. A. A., Ciompi, F., Ghafoorian, M., ... \& Sánchez, C. I. (2017). A survey on deep learning in medical image analysis. \emph{Medical Image Analysis}, 42, 60-88.

\bibitem{pianykh2012digital}
Pianykh, O. S. (2012). \emph{Digital imaging and communications in medicine (DICOM): a practical introduction and survival guide}. Springer Science \& Business Media.

\bibitem{ohif2023}
Open Health Imaging Foundation. (2023). OHIF Medical Imaging Viewer. Retrieved from \url{https://ohif.org/}

\bibitem{cornerstone2023}
Cornerstone.js Community. (2023). Cornerstone.js Medical Imaging Framework. Retrieved from \url{https://cornerstonejs.org/}

\bibitem{monai2023}
MONAI Consortium. (2023). MONAI Label: AI-Assisted Annotation. Retrieved from \url{https://monai.io/label.html}

\bibitem{aalst2003workflow}
Van der Aalst, W. M., \& Van Hee, K. M. (2003). \emph{Workflow management: models, methods, and systems}. MIT press.

\bibitem{hu2014guide}
Hu, V. C., Ferraiolo, D., Kuhn, R., Schnitzer, A., Sandlin, K., Miller, R., \& Scarfone, K. (2014). Guide to attribute based access control (ABAC) definition and considerations. \emph{NIST Special Publication}, 800, 162.

\bibitem{sandhu1996role}
Sandhu, R. S., Coyne, E. J., Feinstein, H. L., \& Youman, C. E. (1996). Role-based access control models. \emph{Computer}, 29(2), 38-47.

\bibitem{monarch2021human}
Monarch, R. (2021). \emph{Human-in-the-Loop Machine Learning: Active learning and annotation for human-centered AI}. Manning Publications.

\bibitem{sheng2008get}
Sheng, V. S., Provost, F., \& Ipeirotis, P. G. (2008). Get another label? improving data quality and data mining using multiple, noisy labelers. \emph{Proceedings of the 14th ACM SIGKDD international conference on Knowledge discovery and data mining}, 614-622.

\end{thebibliography} 